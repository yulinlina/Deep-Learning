{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验 3 - BP 算法\n",
    "\n",
    "## 基本信息\n",
    "\n",
    "* 课程：深度学习引论\n",
    "* 老师：GUO QUAN\n",
    "* 学生：王昊霖\n",
    "* ID：2020141440041\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare trainning data set\n",
    "data_set = np.array([[1,0],[0,1],[1,1],[0,0]])\n",
    "labels = np.array([1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-607-fac142b9f1ba>:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  W = np.array([w1,w2])\n"
     ]
    }
   ],
   "source": [
    "# initial parameters and weights\n",
    "alpha = 0.01\n",
    "epochs = 100000\n",
    "batch_size =2\n",
    "w1 =np.random.randn(3,2)\n",
    "w2 =np.random.randn(1,3)\n",
    "W = np.array([w1,w2])\n",
    "J = np.zeros((1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造数据迭代器，返回 `batch_size` 个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0],\n",
      "       [1, 1]]), array([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "def data_iter(data_set,labels,batch_size):\n",
    "    indices =list(range(len(data_set)))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0,len(indices),batch_size):\n",
    "        batch_indices =indices[i:i+batch_size]\n",
    "        yield data_set[batch_indices],labels[batch_indices]\n",
    "\n",
    "print(next(data_iter(data_set,labels,batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain the forward result\n",
    "a_para = []\n",
    "z_para = []\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(inx):\n",
    "   if inx.any() >= 0: \n",
    "        return 1.0 / (1 + np.exp(-inx))\n",
    "   else:\n",
    "        return np.exp(inx) / (1 + np.exp(inx))\n",
    "\n",
    "def purline(x):\n",
    "    return x\n",
    "\n",
    "def derivative_of_sigmoid(x):\n",
    "    return sigmoid(x)*(1- sigmoid(x))\n",
    "\n",
    "\n",
    "def loss_function(x,y):\n",
    "    return 1./2*(x-y)**2\n",
    "\n",
    "def forward(w,a):\n",
    "    f_list = [sigmoid,purline]\n",
    "    for w_i,f in zip(w,f_list):\n",
    "        z = w_i@a\n",
    "        a = f(z)\n",
    "        z_para.append(copy.deepcopy(z))\n",
    "        a_para.append(copy.deepcopy(a))\n",
    "        \n",
    "    return a\n",
    "\n",
    "def backward(w,delta):\n",
    "    for index,(w_i,z) in enumerate(zip(w[-1:],z_para[-2:])):\n",
    "        F = np.diag(derivative_of_sigmoid(z))\n",
    "        delta = F @ w_i.T@delta\n",
    "    return delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在线BP算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_train():\n",
    "  global W,w1,w2\n",
    "  for t in range(epochs):\n",
    "      for index, (data,label) in enumerate(zip(data_set,labels)):\n",
    "        a_para.append(data.T)\n",
    "        a =  forward(W,data.T)\n",
    "        loss = loss_function(a,label)\n",
    "    \n",
    "        delta_L = (a - label)*(z_para[-1])\n",
    "        delta = backward(W,delta_L)\n",
    "      \n",
    "        dw2 = np.dot(delta_L.reshape((1,1)),np.array(a_para[1]).reshape((1,3)))\n",
    "        dw1 =np.dot(delta.reshape((3,1)),np.array(a_para[0]).reshape((1,2)))\n",
    "\n",
    "        w2 = w2 -alpha*dw2\n",
    "        w1 = w1- alpha*dw1\n",
    "      \n",
    "\n",
    "        W = np.array([w1,w2])\n",
    "\n",
    "        a_para.clear()\n",
    "      if t%1000 == 0:\n",
    "          print(f\"epoch: {t},loss :{loss[0]:.3f}\")\n",
    "# online_train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批处理反向传播算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-612-992ec929c1e0>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  W = np.array([w1,w2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,loss :0.041\n",
      "epoch: 1000,loss :0.193\n",
      "epoch: 2000,loss :0.013\n",
      "epoch: 3000,loss :0.261\n",
      "epoch: 4000,loss :0.006\n",
      "epoch: 5000,loss :0.005\n",
      "epoch: 6000,loss :0.224\n",
      "epoch: 7000,loss :0.051\n",
      "epoch: 8000,loss :0.117\n",
      "epoch: 9000,loss :0.045\n",
      "epoch: 10000,loss :0.042\n",
      "epoch: 11000,loss :0.207\n",
      "epoch: 12000,loss :0.196\n",
      "epoch: 13000,loss :0.070\n",
      "epoch: 14000,loss :0.069\n",
      "epoch: 15000,loss :0.038\n",
      "epoch: 16000,loss :0.179\n",
      "epoch: 17000,loss :0.194\n",
      "epoch: 18000,loss :0.185\n",
      "epoch: 19000,loss :0.038\n",
      "epoch: 20000,loss :0.045\n",
      "epoch: 21000,loss :0.051\n",
      "epoch: 22000,loss :0.035\n",
      "epoch: 23000,loss :0.042\n",
      "epoch: 24000,loss :0.043\n",
      "epoch: 25000,loss :0.044\n",
      "epoch: 26000,loss :0.043\n",
      "epoch: 27000,loss :0.039\n",
      "epoch: 28000,loss :0.036\n",
      "epoch: 29000,loss :0.040\n",
      "epoch: 30000,loss :0.047\n",
      "epoch: 31000,loss :0.043\n",
      "epoch: 32000,loss :0.153\n",
      "epoch: 33000,loss :0.046\n",
      "epoch: 34000,loss :0.029\n",
      "epoch: 35000,loss :0.047\n",
      "epoch: 36000,loss :0.043\n",
      "epoch: 37000,loss :0.046\n",
      "epoch: 38000,loss :0.027\n",
      "epoch: 39000,loss :0.152\n",
      "epoch: 40000,loss :0.042\n",
      "epoch: 41000,loss :0.045\n",
      "epoch: 42000,loss :0.040\n",
      "epoch: 43000,loss :0.047\n",
      "epoch: 44000,loss :0.029\n",
      "epoch: 45000,loss :0.045\n",
      "epoch: 46000,loss :0.039\n",
      "epoch: 47000,loss :0.045\n",
      "epoch: 48000,loss :0.029\n",
      "epoch: 49000,loss :0.146\n",
      "epoch: 50000,loss :0.141\n",
      "epoch: 51000,loss :0.027\n",
      "epoch: 52000,loss :0.037\n",
      "epoch: 53000,loss :0.027\n",
      "epoch: 54000,loss :0.034\n",
      "epoch: 55000,loss :0.050\n",
      "epoch: 56000,loss :0.048\n",
      "epoch: 57000,loss :0.025\n",
      "epoch: 58000,loss :0.035\n",
      "epoch: 59000,loss :0.033\n",
      "epoch: 60000,loss :0.029\n",
      "epoch: 61000,loss :0.024\n",
      "epoch: 62000,loss :0.054\n",
      "epoch: 63000,loss :0.053\n",
      "epoch: 64000,loss :0.028\n",
      "epoch: 65000,loss :0.119\n",
      "epoch: 66000,loss :0.025\n",
      "epoch: 67000,loss :0.034\n",
      "epoch: 68000,loss :0.054\n",
      "epoch: 69000,loss :0.119\n",
      "epoch: 70000,loss :0.118\n",
      "epoch: 71000,loss :0.111\n",
      "epoch: 72000,loss :0.117\n",
      "epoch: 73000,loss :0.023\n",
      "epoch: 74000,loss :0.063\n",
      "epoch: 75000,loss :0.030\n",
      "epoch: 76000,loss :0.065\n",
      "epoch: 77000,loss :0.026\n",
      "epoch: 78000,loss :0.113\n",
      "epoch: 79000,loss :0.069\n",
      "epoch: 80000,loss :0.030\n",
      "epoch: 81000,loss :0.023\n",
      "epoch: 82000,loss :0.068\n",
      "epoch: 83000,loss :0.101\n",
      "epoch: 84000,loss :0.017\n",
      "epoch: 85000,loss :0.019\n",
      "epoch: 86000,loss :0.103\n",
      "epoch: 87000,loss :0.017\n",
      "epoch: 88000,loss :0.082\n",
      "epoch: 89000,loss :0.083\n",
      "epoch: 90000,loss :0.086\n",
      "epoch: 91000,loss :0.085\n",
      "epoch: 92000,loss :0.035\n",
      "epoch: 93000,loss :0.086\n",
      "epoch: 94000,loss :0.035\n",
      "epoch: 95000,loss :0.032\n",
      "epoch: 96000,loss :0.028\n",
      "epoch: 97000,loss :0.104\n",
      "epoch: 98000,loss :0.077\n",
      "epoch: 99000,loss :0.046\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    J = [np.zeros_like(w1),np.zeros_like(w2)]\n",
    "    for minbatch in data_iter(data_set,labels,batch_size):\n",
    "      for data,label in zip(minbatch[0],minbatch[1]):\n",
    "        a_para.append(data.T)\n",
    "        a =  forward(W,data.T)\n",
    "        loss = loss_function(a,label)\n",
    "    \n",
    "        delta_L = (a - label)*(z_para[-1])\n",
    "        delta = backward(W,delta_L)\n",
    "      \n",
    "        dw2 = np.dot(delta_L.reshape((1,1)),np.array(a_para[1]).reshape((1,3)))\n",
    "        dw1 =np.dot(delta.reshape((3,1)),np.array(a_para[0]).reshape((1,2)))\n",
    "\n",
    "        J[1]+=dw2\n",
    "        J[0]+=dw1\n",
    "      w2 = w2 -alpha*J[1]/batch_size\n",
    "      w1 = w1- alpha*J[0]/batch_size\n",
    "    \n",
    "\n",
    "    W = np.array([w1,w2])\n",
    "\n",
    "    a_para.clear()\n",
    "    if t%1000 == 0:\n",
    "        print(f\"epoch: {t},loss :{loss[0]:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple ([1 0],1) is classified  1\n",
      "simple ([0 1],1) is classified  1\n",
      "simple ([1 1],0) is classified  0\n",
      "simple ([0 0],0) is classified  0\n"
     ]
    }
   ],
   "source": [
    "for index, (data,label) in enumerate(zip(data_set,labels)):\n",
    "    a =  forward(W,data.T)\n",
    "    print(f\"simple ({data},{label}) is classified  {round(a[0])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
